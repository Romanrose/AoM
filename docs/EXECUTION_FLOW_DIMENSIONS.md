# AoMä»£ç æ‰§è¡Œé¡ºåºä¸æ•°æ®ç»´åº¦å˜åŒ–è¯¦è§£

> æœ¬æ–‡æ¡£æŒ‰ç…§ä»£ç å®é™…è¿è¡Œé¡ºåºï¼Œé€è¡Œåˆ†ææ•°æ®ç»´åº¦åœ¨å„æ¨¡å—é—´çš„ä¼ é€’å’Œå˜æ¢è¿‡ç¨‹ã€‚

---

## ğŸ“Š æ‰§è¡Œæµç¨‹æ¦‚è§ˆ

```mermaid
graph TD
    A[1. åˆå§‹åŒ–é˜¶æ®µ] --> B[2. æ•°æ®åŠ è½½]
    B --> C[3. Tokenizer]
    C --> D[4. æ¨¡å‹åˆå§‹åŒ–]
    D --> E[5. å‰å‘ä¼ æ’­]
    E --> F[6. AÂ³Mæ¨¡å—]
    F --> G[7. AG-GCNæ¨¡å—]
    G --> H[8. Decoder]
    H --> I[9. é¢„æµ‹è¾“å‡º]
```

---

## 1. åˆå§‹åŒ–é˜¶æ®µ (MAESC_training.py)

### 1.1 ç¯å¢ƒå‚æ•°åˆå§‹åŒ–

```python
# MAESC_training.py: 20-50
parser.add_argument('--dataset', default='twitter15')
parser.add_argument('--batch_size', default=16)
parser.add_argument('--max_len', default=10)
...

args = parser.parse_args()
```

**æ•°æ®ç»´åº¦å˜åŒ–**: æ— 

---

## 2. æ•°æ®åŠ è½½é˜¶æ®µ (dataset.py)

### 2.1 æ•°æ®é›†åˆå§‹åŒ–

```python
# dataset.py: __init__æ–¹æ³•
def __init__(self, examples, tokenizer, args, mode='train'):
    self.examples = examples  # List[Exampleå¯¹è±¡]
    self.tokenizer = tokenizer
    self.args = args
    self.mode = mode
```

**æ•°æ®ç»´åº¦å˜åŒ–**:
```
è¾“å…¥: examples = [ex1, ex2, ..., exN]  # Nä¸ªæ ·æœ¬
è¾“å‡º: self.examples = List[Example]    # [N]
```

### 2.2 æ‰¹å¤„ç†æ•°æ®å‡†å¤‡

```python
# dataset.py: __getitem__æ–¹æ³•
def __getitem__(self, index):
    example = self.examples[index]

    # 1. å¤„ç†æ–‡æœ¬token
    input_ids = example.input_ids  # List[int]

    # 2. å¤„ç†å›¾åƒç‰¹å¾
    image_features = example.image_features  # List[List[float]]

    # 3. å¤„ç†æ ‡ç­¾
    labels = example.labels  # List[int]

    # 4. åˆ›å»ºmask
    mask = example.mask  # List[int]

    return {
        'input_ids': torch.tensor(input_ids, dtype=torch.long),
        'image_features': image_features,
        'labels': torch.tensor(labels, dtype=torch.long),
        'mask': torch.tensor(mask, dtype=torch.long),
    }
```

**æ•°æ®ç»´åº¦å˜åŒ–**:
```
è¾“å…¥: ä¸€ä¸ªæ ·æœ¬
â”œâ”€â”€ input_ids: [seq_len] = [66]  # 51å›¾åƒ+15æ–‡æœ¬
â”œâ”€â”€ image_features: [51, 2048]   # 51ä¸ªå›¾åƒå—ï¼Œ2048ç»´ROIç‰¹å¾
â”œâ”€â”€ labels: [target_len]         # ç›®æ ‡åºåˆ—é•¿åº¦
â””â”€â”€ mask: [seq_len] = [66]       # æ³¨æ„åŠ›æ©ç 

è¾“å‡º: ä¸€ä¸ªæ‰¹æ¬¡ (batch_size=16)
input_ids: [B, 66]
image_features: [B, 51, 2048]
labels: [B, target_len]
mask: [B, 66]
```

---

## 3. Tokenizeråˆå§‹åŒ– (tokenization_new.py)

### 3.1 Tokenizerç±»åˆå§‹åŒ–

```python
# tokenization_new.py: __init__æ–¹æ³•
def __init__(self, args):
    self._base_tokenizer = BartTokenizer.from_pretrained(args.bart_model)
    self.sentinet_on = args.sentinet_on

    # åŠ è½½SenticNetè¯å…¸
    if self.sentinet_on:
        path = os.path.join(os.path.dirname(__file__), '../senticnet_word.txt')
        self.senticNet = {}
        fp = open(path, 'r')
        for line in fp:
            word, sentic = line.split('\t')
            self.senticNet[word] = sentic
```

**æ•°æ®ç»´åº¦å˜åŒ–**:
```
è¾“å…¥: args (é…ç½®å‚æ•°)
è¾“å‡º: self.senticNet = {word: sentic_score}
      ç»´åº¦: Dict[str, str] = å¤§çº¦10,000ä¸ªè¯æ¡
```

---

## 4. æ¨¡å‹åˆå§‹åŒ– (MAESC_model.py)

### 4.1 æ¨¡å‹æ„å»º

```python
# MAESC_model.py: build_modelæ–¹æ³•
def build_model(self, args, bart_model, tokenizer, label_ids, config):
    # åŠ è½½BARTé¢„è®­ç»ƒæ¨¡å‹
    model = BartModel.from_pretrained(bart_model)

    # è°ƒæ•´è¯è¡¨å¤§å°
    model.resize_token_embeddings(len(tokenizer.unique_no_split_tokens) + num_tokens)

    # åˆ›å»ºå¤šæ¨¡æ€ç¼–ç å™¨
    multimodal_encoder = MultiModalBartEncoder(config, encoder, ...)

    return (multimodal_encoder, decoder)
```

**æ•°æ®ç»´åº¦å˜åŒ–**:
```
è¾“å…¥: bart_modelæƒé‡
è¾“å‡º: å¤šæ¨¡æ€æ¨¡å‹
      - encoder: BARTç¼–ç å™¨
      - decoder: BARTè§£ç å™¨
      - æ‰€æœ‰çº¿æ€§å±‚: åˆå§‹åŒ–ä¸º[768, 768]
```

### 4.2 æŸå¤±å‡½æ•°åˆå§‹åŒ–

```python
# MAESC_model.py: åˆå§‹åŒ–éƒ¨åˆ†
self.senti_linear = nn.Linear(768, 768)         # W_S
self.context_linear = nn.Linear(768, 768)      # W_context
self.noun_linear = nn.Linear(768, 768)         # W_CA
self.multi_linear = nn.Linear(768, 768)        # W_H
self.att_linear = nn.Linear(768*2, 1)          # W_Î±
self.alpha_linear1 = nn.Linear(768, 768)       # W_1
self.alpha_linear2 = nn.Linear(768, 768)       # W_2
self.linear = nn.Linear(768*2, 1)              # W_Î²
```

**æ•°æ®ç»´åº¦å˜åŒ–**:
```
æ— æ•°æ®æµåŠ¨ï¼Œä»…å‚æ•°ç»´åº¦:
æ‰€æœ‰Linearå±‚æƒé‡åˆå§‹åŒ–ä¸º:
- è¾“å…¥768ç»´ â†’ è¾“å‡º768ç»´: [768, 768]
- è¾“å…¥1536ç»´ â†’ è¾“å‡º1ç»´: [1, 1536]
- è¾“å…¥1ç»´ â†’ è¾“å‡º768ç»´: [768, 1]
```

---

## 5. å‰å‘ä¼ æ’­å‡†å¤‡é˜¶æ®µ

### 5.1 è¾“å…¥æ•°æ®å‡†å¤‡

```python
# MAESC_model.py: prepare_stateæ–¹æ³•
def prepare_state(self, input_ids, image_features, attention_mask, ...):
    # åˆ†ç¦»å›¾åƒå’Œæ–‡æœ¬éƒ¨åˆ†
    img_ids = input_ids[:, :51]    # å›¾åƒtoken IDs
    text_ids = input_ids[:, 51:]   # æ–‡æœ¬token IDs

    # noun_mask: æ ‡è¯†å“ªäº›ä½ç½®æ˜¯åè¯
    # sentiment_value: æ–‡æœ¬tokençš„æƒ…æ„Ÿåˆ†æ•°
```

**æ•°æ®ç»´åº¦å˜åŒ–**:
```
è¾“å…¥: input_ids: [B, 66]
è¾“å‡º:
â”œâ”€â”€ img_ids: [B, 51]
â”œâ”€â”€ text_ids: [B, 15]
â”œâ”€â”€ img_features: [B, 51, 2048]
â””â”€â”€ text_features: [B, 15]
```

### 5.2 ç¼–ç å™¨å‰å‘ä¼ æ’­

```python
# MAESC_model.py: prepare_stateæ–¹æ³•
dict = self.encoder(
    input_ids=input_ids,
    image_features=image_features,
    attention_mask=attention_mask,
    output_hidden_states=True,
    return_dict=True)

encoder_outputs = dict.last_hidden_state  # [B, 66, 768]
hidden_states = dict.hidden_states        # List[Tensors]
```

**æ•°æ®ç»´åº¦å˜åŒ–**:
```
è¾“å…¥:
â”œâ”€â”€ input_ids: [B, 66]
â”œâ”€â”€ image_features: [B, 51, 2048]
â””â”€â”€ attention_mask: [B, 66]

è¾“å‡º:
â”œâ”€â”€ encoder_outputs: [B, 66, 768]  # æœ€åä¸€å±‚éšè—çŠ¶æ€
â””â”€â”€ hidden_states: List[13å±‚]       # æ¯å±‚: [B, 66, 768]
```

---

## 6. AÂ³Mæ¨¡å—æ‰§è¡Œ (noun_attentionæ–¹æ³•)

### 6.1 åè¯ç‰¹å¾æå– (get_noun_embed)

```python
# MAESC_model.py: get_noun_embed
noun_mask = noun_mask.cpu()
noun_num = [x.numpy().tolist().count(1) for x in noun_mask]
noun_position = [np.where(np.array(x)==1)[0].tolist() for x in noun_mask]

max_noun_num = max(noun_num)
noun_position = torch.tensor(noun_position).to(self.mydevice)
noun_embed = torch.zeros(feature.shape[0], max_noun_num, feature.shape[-1]).to(self.mydevice)

for i in range(len(feature)):
    noun_embed[i] = torch.index_select(feature[i], dim=0, index=noun_position[i])
```

**æ•°æ®ç»´åº¦å˜åŒ–**:
```
è¾“å…¥:
â”œâ”€â”€ feature (encoder_outputs): [B, 66, 768]
â””â”€â”€ noun_mask: [B, 66]  # æ ‡è¯†åè¯ä½ç½®

å¤„ç†è¿‡ç¨‹:
noun_mask â†’ noun_position: [B, 15] (åªä¿ç•™æ–‡æœ¬éƒ¨åˆ†åè¯ä½ç½®)
noun_position â†’ [B, max_noun_num] (paddingåˆ°æœ€å¤§é•¿åº¦)

è¾“å‡º: noun_embed: [B, L, 768]
     å…¶ä¸­L = max_noun_num (æ¯ä¸ªbatchçš„æœ€å¤§åè¯æ•°)
```

### 6.2 AÂ³Mæ³¨æ„åŠ›è®¡ç®— (mode='cat')

```python
# MAESC_model.py: noun_attentionæ–¹æ³•
# æ­¥éª¤1: ç‰¹å¾å¤åˆ¶æ‰©å±•
multi_features_rep = encoder_outputs.unsqueeze(2).repeat(1, 1, noun_embed.shape[1], 1)
# [B, 66, 768] â†’ [B, 66, 1, 768] â†’ [B, 66, L, 768]

noun_features_rep = noun_embed.unsqueeze(1).repeat(1, encoder_outputs.shape[1], 1, 1)
# [B, L, 768] â†’ [B, 1, L, 768] â†’ [B, 66, L, 768]

# æ­¥éª¤2: çº¿æ€§å˜æ¢
noun_features_rep = self.noun_linear(noun_features_rep)  # W_CA
# [B, 66, L, 768] â†’ [B, 66, L, 768]

multi_features_rep = self.multi_linear(multi_features_rep)  # W_H
# [B, 66, L, 768] â†’ [B, 66, L, 768]

# æ­¥éª¤3: ç‰¹å¾æ‹¼æ¥
concat_features = torch.tanh(torch.cat([noun_features_rep, multi_features_rep], dim=-1))
# [B, 66, L, 768] + [B, 66, L, 768] â†’ [B, 66, L, 1536]

# æ­¥éª¤4: æ³¨æ„åŠ›æƒé‡è®¡ç®—
att = torch.softmax(self.att_linear(concat_features).squeeze(-1), dim=-1)
# concat_features: [B, 66, L, 1536]
# att_linear: [1, 1536] â†’ [B, 66, L, 1]
# .squeeze(-1): [B, 66, L]
# softmax: [B, 66, L] = Î±_t

# æ­¥éª¤5: æ–¹é¢ç›¸å…³ç‰¹å¾
att_features = torch.matmul(att, noun_embed)
# [B, 66, L] @ [B, L, 768] â†’ [B, 66, 768] = h_t^A

# æ­¥éª¤6: èåˆç³»æ•°
alpha = torch.sigmoid(self.linear(
    torch.cat([self.alpha_linear1(encoder_outputs),
               self.alpha_linear2(att_features)], dim=-1)))
# encoder_outputs: [B, 66, 768]
# alpha_linear1: [B, 66, 768]
# att_features: [B, 66, 768]
# alpha_linear2: [B, 66, 768]
# concat: [B, 66, 1536]
# linear: [1, 1536] â†’ [B, 66, 1]
# sigmoid: [B, 66, 1] = Î²_t

# æ­¥éª¤7: æœ€ç»ˆå¯¹é½ç‰¹å¾
alpha = alpha.repeat(1, 1, 768)  # å¹¿æ’­
# [B, 66, 1] â†’ [B, 66, 768]

encoder_outputs = torch.mul(1-alpha, encoder_outputs) + torch.mul(alpha, att_features)
# (1-Î±) Ã— h_t + Î± Ã— h_t^A
# [B, 66, 768] = Ä¥_t
```

**å®Œæ•´æ•°æ®ç»´åº¦å˜åŒ–æµç¨‹**:
```
è¾“å…¥: encoder_outputs: [B, 66, 768]
      noun_embed: [B, L, 768]

æ­¥éª¤1-3: multi_features_rep: [B, 66, L, 768]
         noun_features_rep: [B, 66, L, 768]
         concat_features: [B, 66, L, 1536]

æ­¥éª¤4: att: [B, 66, L]

æ­¥éª¤5: att_features: [B, 66, 768] = h_t^A

æ­¥éª¤6: alpha: [B, 66, 1] â†’ [B, 66, 768]

æ­¥éª¤7: encoder_outputs: [B, 66, 768] = Ä¥_t (æœ€ç»ˆè¾“å‡º)
```

---

## 7. AG-GCNæ¨¡å—æ‰§è¡Œ (multimodal_GCNæ–¹æ³•)

### 7.1 å¤šæ¨¡æ€ç‰¹å¾åˆ†ç¦»

```python
# MAESC_model.py: multimodal_GCN
img_feature = encoder_outputs[:, :51, :]  # [B, 51, 768]
text_feature = encoder_outputs[:, 51:, :] # [B, 15, 768]
```

**æ•°æ®ç»´åº¦å˜åŒ–**:
```
è¾“å…¥: encoder_outputs: [B, 66, 768] = Ä¥_t
è¾“å‡º:
â”œâ”€â”€ img_feature: [B, 51, 768]  # å›¾åƒç‰¹å¾
â””â”€â”€ text_feature: [B, 15, 768]  # æ–‡æœ¬ç‰¹å¾
```

### 7.2 ä¾èµ–çŸ©é˜µåˆå§‹åŒ–

```python
# MAESC_model.py: multimodal_GCN
new_dependency_matrix = torch.zeros([B, 66, 66], dtype=torch.float).to(encoder_outputs.device)
# [B, 66, 66] = D (å¤šæ¨¡æ€ä¾èµ–çŸ©é˜µ)

# è®¾ç½®å¯¹è§’çº¿ä¸º1 (è‡ªç¯)
for i in range(new_dependency_matrix.shape[1]):
    new_dependency_matrix[:, i, i] = 1
```

**æ•°æ®ç»´åº¦å˜åŒ–**:
```
è¾“å‡º: new_dependency_matrix: [B, 66, 66] = D
```

### 7.3 æ–‡æœ¬-æ–‡æœ¬ä¾èµ–è®¡ç®—

```python
# MAESC_model.py: multimodal_GCN
text_feature_extend1 = text_feature.unsqueeze(1).repeat(1, 15, 1, 1)
text_feature_extend2 = text_feature.unsqueeze(2).repeat(1, 1, 15, 1)
text_sim = torch.cosine_similarity(text_feature_extend1, text_feature_extend2, dim=-1)
# [B, 15, 768] â†’ [B, 15, 15, 768]
# ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—
# è¾“å‡º: text_sim: [B, 15, 15]

new_dependency_matrix[:, 51:, 51:] = dependency_matrix * text_sim
# [B, 15, 15] å¡«å…¥Dçš„æ–‡æœ¬-æ–‡æœ¬å­çŸ©é˜µ
```

**æ•°æ®ç»´åº¦å˜åŒ–**:
```
è¾“å…¥: text_feature: [B, 15, 768]
è¿‡ç¨‹:
â”œâ”€â”€ text_feature_extend1: [B, 15, 15, 768]
â”œâ”€â”€ text_feature_extend2: [B, 15, 15, 768]
â””â”€â”€ text_sim: [B, 15, 15]

è¾“å‡º: D_TTå­çŸ©é˜µ: [B, 15, 15]
```

### 7.4 å›¾åƒ-æ–‡æœ¬ä¾èµ–è®¡ç®—

```python
# MAESC_model.py: multimodal_GCN
img_feature_extend = img_feature.unsqueeze(2).repeat(1, 1, text_feature.shape[1], 1)
text_feature_extend = text_feature.unsqueeze(1).repeat(1, img_feature.shape[1], 1, 1)
sim = torch.cosine_similarity(img_feature_extend, text_feature_extend, dim=-1)
# [B, 51, 768] â†’ [B, 51, 15, 768]
# [B, 15, 768] â†’ [B, 51, 15, 768]
# ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—
# è¾“å‡º: sim: [B, 51, 15]

noun_mask = noun_mask[:, 51:].unsqueeze(1).repeat(1, sim.shape[1], 1)
sim = sim * noun_mask  # åªä¿ç•™åè¯ç›¸å…³çš„å›¾åƒ-æ–‡æœ¬ä¾èµ–

new_dependency_matrix[:, :51, 51:] = sim  # D_VT
new_dependency_matrix[:, 51:, :51] = torch.transpose(sim, 1, 2)  # D_TV
```

**æ•°æ®ç»´åº¦å˜åŒ–**:
```
è¾“å…¥: img_feature: [B, 51, 768]
      text_feature: [B, 15, 768]

è¿‡ç¨‹:
â”œâ”€â”€ img_feature_extend: [B, 51, 15, 768]
â”œâ”€â”€ text_feature_extend: [B, 51, 15, 768]
â”œâ”€â”€ sim: [B, 51, 15] (å›¾åƒ-æ–‡æœ¬ç›¸ä¼¼åº¦)
â””â”€â”€ noun_mask: [B, 15] â†’ [B, 51, 15] (è¿‡æ»¤)

è¾“å‡º:
â”œâ”€â”€ D_VTå­çŸ©é˜µ: [B, 51, 15]
â””â”€â”€ D_TVå­çŸ©é˜µ: [B, 15, 51]
```

### 7.5 æƒ…æ„Ÿç‰¹å¾å¤„ç†

```python
# MAESC_model.py: multimodal_GCN
# SenticNetæƒ…æ„Ÿåˆ†æ•°
sentiment_value = nn.ZeroPad2d(padding=(51, 0, 0, 0))(sentiment_value)
# [B, 15] â†’ [B, 66] (å·¦ä¾§å¡«å……51ä¸ª0)

sentiment_value = sentiment_value.unsqueeze(-1)
# [B, 66] â†’ [B, 66, 1]

sentiment_feature = self.senti_value_linear(sentiment_value)
# [B, 66, 1] â†’ senti_value_linear â†’ [B, 66, 768] = s_i
```

**æ•°æ®ç»´åº¦å˜åŒ–**:
```
è¾“å…¥: sentiment_value: [B, 15] (æ¥è‡ªSenticNet)
è¿‡ç¨‹:
â”œâ”€â”€ ZeroPad2d: [B, 15] â†’ [B, 66]
â”œâ”€â”€ unsqueeze: [B, 66] â†’ [B, 66, 1]
â””â”€â”€ Linear: [B, 66, 1] @ [768, 1]^T â†’ [B, 66, 768]

è¾“å‡º: sentiment_feature: [B, 66, 768] = s_i
```

### 7.6 æƒ…æ„Ÿ-è¯­ä¹‰èåˆ

```python
# MAESC_model.py: multimodal_GCN
context_feature = self.context_linear(encoder_outputs + sentiment_feature)
# encoder_outputs: [B, 66, 768] = Ä¥_i
# sentiment_feature: [B, 66, 768] = s_i
# åŠ æ³•: [B, 66, 768] + [B, 66, 768] = [B, 66, 768]
# Linear: [B, 66, 768] â†’ [B, 66, 768]
```

**æ•°æ®ç»´åº¦å˜åŒ–**:
```
è¾“å…¥:
â”œâ”€â”€ encoder_outputs: [B, 66, 768]
â””â”€â”€ sentiment_feature: [B, 66, 768]

è¿‡ç¨‹: [B, 66, 768] + [B, 66, 768] â†’ [B, 66, 768]
      context_linear: [B, 66, 768] â†’ [B, 66, 768]

è¾“å‡º: context_feature: [B, 66, 768] = h_i^S
```

### 7.7 å›¾å·ç§¯è®¡ç®— (GCNå±‚)

```python
# GCN.py: forwardæ–¹æ³•
context_feature = self.context_gcn(context_feature, context_dependency_matrix, attention_mask)

# GCNå†…éƒ¨å®ç°:
def forward(self, inputs, adj, mask=None):
    # inputs: [B, 66, 768] = h_{i,l-1}^S
    # adj: [B, 66, 66] = new_dependency_matrix

    # æ­¥éª¤1: çº¿æ€§å˜æ¢
    support = torch.matmul(inputs, self.weight)
    # [B, 66, 768] @ [768, 768] â†’ [B, 66, 768]

    # æ­¥éª¤2: å›¾å·ç§¯
    output = torch.matmul(adj, support)
    # [B, 66, 66] @ [B, 66, 768] â†’ [B, 66, 768]

    # æ­¥éª¤3: æ¿€æ´»
    output = self.act(output + self.bias)  # ReLU
    # [B, 66, 768] = h_{i,l}^S

    return output
```

**æ•°æ®ç»´åº¦å˜åŒ–**:
```
è¾“å…¥:
â”œâ”€â”€ context_feature: [B, 66, 768] = h_{i,l-1}^S
â””â”€â”€ context_dependency_matrix: [B, 66, 66] = A

GCNå±‚1:
â”œâ”€â”€ support: [B, 66, 768]
â””â”€â”€ output: [B, 66, 768] = h_{i,1}^S

GCNå±‚2 (æœ€ç»ˆè¾“å‡º):
context_gcn_output: [B, 66, 768] = Ä¤^S
```

### 7.8 æœ€ç»ˆç‰¹å¾èåˆ

```python
# MAESC_model.py: multimodal_GCN
mix_feature = self.gcn_proportion * context_feature + encoder_outputs
# gcn_proportion: 0.5 (é»˜è®¤)
# 0.5 Ã— Ä¤^S + 1.0 Ã— Ä¤ = HÌƒ
```

**æ•°æ®ç»´åº¦å˜åŒ–**:
```
è¾“å…¥:
â”œâ”€â”€ context_feature (GCNè¾“å‡º): [B, 66, 768] = Ä¤^S
â””â”€â”€ encoder_outputs: [B, 66, 768] = Ä¤

è¿‡ç¨‹: 0.5 Ã— [B, 66, 768] + 1.0 Ã— [B, 66, 768] â†’ [B, 66, 768]

è¾“å‡º: mix_feature: [B, 66, 768] = HÌƒ
```

---

## 8. Decoderå‰å‘ä¼ æ’­

### 8.1 è§£ç å™¨è¾“å…¥å‡†å¤‡

```python
# modules.py: MultiModalBartDecoder_span
# tokens: [B, tgt_len] (ç›®æ ‡åºåˆ—)

cumsum = tokens.eq(1).flip(dims=[1]).cumsum(dim=-1)
tgt_pad_mask = cumsum.flip(dims=[1]).ne(cumsum[:, -1:])
# [B, tgt_len] (padding mask)
```

**æ•°æ®ç»´åº¦å˜åŒ–**:
```
è¾“å…¥: tokens: [B, tgt_len]
è¾“å‡º: tgt_pad_mask: [B, tgt_len]
```

### 8.2 BARTè§£ç å™¨å‰å‘ä¼ æ’­

```python
# modules.py: MultiModalBartDecoder_span.forward
dict = self.decoder(input_ids=tokens,
                    encoder_hidden_states=mix_feature,
                    encoder_padding_mask=encoder_pad_mask,
                    decoder_padding_mask=decoder_pad_mask,
                    decoder_causal_mask=self.causal_masks[:tokens.size(1), :tokens.size(1)],
                    return_dict=True)

hidden_state = dict.last_hidden_state
# [B, tgt_len, 768] = h_t^d
```

**æ•°æ®ç»´åº¦å˜åŒ–**:
```
è¾“å…¥:
â”œâ”€â”€ tokens: [B, tgt_len]
â”œâ”€â”€ mix_feature (encoderè¾“å‡º): [B, 66, 768] = HÌƒ
â”œâ”€â”€ encoder_pad_mask: [B, 66]
â”œâ”€â”€ decoder_pad_mask: [B, tgt_len]
â””â”€â”€ causal_mask: [tgt_len, tgt_len]

BART Decoderè¾“å‡º:
â””â”€â”€ hidden_state: [B, tgt_len, 768] = h_t^d
```

---

## 9. é¢„æµ‹è¾“å‡ºé˜¶æ®µ

### 9.1 æƒ…æ„Ÿæ ‡ç­¾é¢„æµ‹

```python
# modules.py: MultiModalBartDecoder_span.forward
tag_scores = F.linear(
    hidden_state,
    self.dropout_layer(
        self.decoder.embed_tokens.weight[self.label_start_id:self.label_start_id + 3]))
# hidden_state: [B, tgt_len, 768]
# embed_tokens.weight[labeléƒ¨åˆ†]: [3, 768] (POS, NEU, NEG)
# F.linear: [B, tgt_len, 768] @ [3, 768]^T â†’ [B, tgt_len, 3]

logits[:, :, 3:self.src_start_index] = tag_scores
# [B, tgt_len, num_classes] = P(y_t)
```

**æ•°æ®ç»´åº¦å˜åŒ–**:
```
è¾“å…¥:
â”œâ”€â”€ hidden_state: [B, tgt_len, 768] = h_t^d
â””â”€â”€ label_embeddings: [3, 768] (POS, NEU, NEG)

è¿‡ç¨‹: [B, tgt_len, 768] @ [3, 768]^T â†’ [B, tgt_len, 3]

è¾“å‡º: logits: [B, tgt_len, num_classes] = P(y_t)
     å…¶ä¸­3ä¸ªæƒ…æ„Ÿç±»åˆ«: POS, NEU, NEG
```

### 9.2 æœ€ç»ˆè¾“å‡º

```python
# è¿”å›: logits: [B, tgt_len, num_classes]
# æŸå¤±è®¡ç®—: CrossEntropyLoss(logits, targets)
```

---

## å®Œæ•´æ•°æ®æµæ€»ç»“

### æ‰¹æ¬¡æ•°æ®æµåŠ¨

```
è¾“å…¥æ‰¹æ¬¡ (batch_size=B=16):
â”œâ”€â”€ input_ids: [B, 66]
â”œâ”€â”€ image_features: [B, 51, 2048]
â””â”€â”€ labels: [B, tgt_len]

ç¼–ç å™¨è¾“å‡º:
encoder_outputs: [B, 66, 768]

AÂ³Mæ¨¡å—:
noun_embed: [B, L, 768]
â†’ att: [B, 66, L]
â†’ att_features: [B, 66, 768]
â†’ alpha: [B, 66, 768]
â†’ encoder_outputs: [B, 66, 768] = Ä¥_t

AG-GCNæ¨¡å—:
img_feature: [B, 51, 768]
text_feature: [B, 15, 768]
â†’ sim: [B, 51, 15]
â†’ dependency_matrix: [B, 66, 66]
â†’ sentiment_feature: [B, 66, 768]
â†’ context_feature: [B, 66, 768]
â†’ GCN_output: [B, 66, 768] = Ä¤^S
â†’ mix_feature: [B, 66, 768] = HÌƒ

è§£ç å™¨è¾“å‡º:
hidden_state: [B, tgt_len, 768] = h_t^d

æœ€ç»ˆè¾“å‡º:
logits: [B, tgt_len, num_classes] = P(y_t)
```

### å…³é”®ç»´åº¦å¸¸æ•°

| åç§° | å€¼ | è¯´æ˜ |
|------|----|----|
| B | 16 | æ‰¹æ¬¡å¤§å° |
| seq_len | 66 | è¾“å…¥åºåˆ—é•¿åº¦ (51å›¾åƒ+15æ–‡æœ¬) |
| img_len | 51 | å›¾åƒpatchæ•°é‡ |
| text_len | 15 | æ–‡æœ¬tokenæ•°é‡ |
| hidden_size | 768 | éšè—å±‚ç»´åº¦ |
| noun_len | L | åè¯æ•°é‡ (åŠ¨æ€ï¼Œæœ€å¤§çº¦10) |
| tgt_len | 10 | ç›®æ ‡åºåˆ—é•¿åº¦ |
| num_classes | 50265 | è¯è¡¨å¤§å° |

---

**åˆ†æå®Œæˆæ—¶é—´**: 2025-11-13
**ä»£ç æ‰§è¡Œé¡ºåºç»´åº¦è·Ÿè¸ª**: âœ…