# AoMè®ºæ–‡å…¬å¼ä¸ä»£ç ç»´åº¦å¯¹åº”åˆ†æ

> æœ¬æ–‡æ¡£è¯¦ç»†åˆ†æAoMè®ºæ–‡ä¸­æ ¸å¿ƒå…¬å¼æ¨å¯¼çš„æ•°æ®ç»´åº¦å˜åŒ–åœ¨ä»£ç ä¸­çš„å…·ä½“å®ç°ã€‚

---

## ğŸ“Š æ•°æ®æµæ€»è§ˆ

```python
è¾“å…¥æ•°æ® â†’ BARTç¼–ç  â†’ AÂ³Mæ¨¡å— â†’ AG-GCNæ¨¡å— â†’ é¢„æµ‹è¾“å‡º
  â†“          â†“          â†“          â†“          â†“
[batch,   [batch,    [batch,    [batch,    [batch,
 seq_len, seq_len,   seq_len,   seq_len,   seq_len,
  768]     768]       768]       768]       768]
```

---

## ä¸€ã€AÂ³Mæ¨¡å— - æ•°æ®ç»´åº¦å˜æ¢è¯¦è§£

### 1. è¾“å…¥æ•°æ®ç»´åº¦

```python
encoder_outputs: [batch_size, seq_len, hidden_size]
                = [B, 66, 768]  # seq_len=66 (51å›¾åƒ+15æ–‡æœ¬token)
noun_embed:     [batch_size, max_noun_num, hidden_size]
                = [B, L, 768]   # Lä¸ºæœ€å¤§åè¯æ•°é‡
```

**ä»£ç ä½ç½®**: `MAESC_model.py:141-159` (`get_noun_embed`æ–¹æ³•)

### 2. åè¯ç‰¹å¾æå– - å€™é€‰æ–¹é¢ç‰¹å¾ H^CA

#### è®ºæ–‡å…¬å¼
```
H^CA = {h_1^CA, h_2^CA, ..., h_l^CA} âˆˆ â„^(dÃ—l)
```

#### ä»£ç å®ç°
```python
# MAESC_model.py:150-158
noun_embed = torch.zeros(feature.shape[0], max_noun_num, feature.shape[-1]).to(self.mydevice)
# ç»´åº¦: [B, max_noun_num, 768] = [B, L, d]

for i in range(len(feature)):
    noun_embed[i] = torch.index_select(feature[i], dim=0, index=noun_position[i])
    noun_embed[i, noun_num[i]:] = torch.zeros(max_noun_num-noun_num[i], feature.shape[-1])
```

**ç»´åº¦å˜åŒ–**:
- `feature`: `[B, 66, 768]` â†’ æå–åè¯ä½ç½®ç‰¹å¾
- `noun_embed`: `[B, max_noun_num, 768]` = `[B, L, d]`

---

### 3. ç»¼åˆç‰¹å¾ Z_t è®¡ç®— (å…¬å¼1)

#### è®ºæ–‡å…¬å¼
```
Z_t = tanh((W_CA H^CA + b_CA) âŠ• (W_H h_t + b_H))
ç»´åº¦: tanh([2dÃ—l]) âˆˆ â„^(2dÃ—l)
```

#### ä»£ç å®ç°
```python
# MAESC_model.py:208-213 (noun_attentionæ–¹æ³•)
# 1. ç‰¹å¾å¤åˆ¶æ‰©å±•
multi_features_rep = encoder_outputs.unsqueeze(2).repeat(1, 1, noun_embed.shape[1], 1)
# ç»´åº¦: [B, 66, 1, 768] â†’ [B, 66, L, 768]

noun_features_rep = noun_embed.unsqueeze(1).repeat(1, encoder_outputs.shape[1], 1, 1)
# ç»´åº¦: [B, 1, L, 768] â†’ [B, 66, L, 768]

# 2. çº¿æ€§å˜æ¢
noun_features_rep = self.noun_linear(noun_features_rep)  # W_CA
multi_features_rep = self.multi_linear(multi_features_rep)  # W_H
# ç»´åº¦: [B, 66, L, 768] â†’ çº¿æ€§å˜æ¢åä»ä¸º[...]

# 3. æ‹¼æ¥å’Œæ¿€æ´»
concat_features = torch.tanh(torch.cat([noun_features_rep, multi_features_rep], dim=-1))
# ç»´åº¦: torch.cat([B,66,L,768], [B,66,L,768], dim=-1) = [B,66,L,1536] = [B,66,L,2d]
```

**ç»´åº¦å˜åŒ–**:
- `encoder_outputs`: `[B, 66, 768]` â†’ unsqueezeâ†’ `[B, 66, 1, 768]` â†’ repeatâ†’ `[B, 66, L, 768]`
- `noun_embed`: `[B, L, 768]` â†’ unsqueezeâ†’ `[B, 1, L, 768]` â†’ repeatâ†’ `[B, 66, L, 768]`
- `concat_features`: `[B, 66, L, 1536]` = `[B, 66, L, 2d]`

---

### 4. æ³¨æ„åŠ›åˆ†å¸ƒ Î±_t è®¡ç®— (å…¬å¼2)

#### è®ºæ–‡å…¬å¼
```
Î±_t = softmax(W_Î± Z_t + b_Î±)
ç»´åº¦: softmax([1Ã—l]) âˆˆ â„^l
```

#### ä»£ç å®ç°
```python
# MAESC_model.py:214
att = torch.softmax(self.att_linear(concat_features).squeeze(-1), dim=-1)
# ç»´åº¦:
# concat_features: [B, 66, L, 1536]
# att_linear(concat_features): [B, 66, L, 1]
# .squeeze(-1): [B, 66, L]
# softmax(dim=-1): [B, 66, L]
```

**ç»´åº¦å˜åŒ–**:
- `concat_features`: `[B, 66, L, 1536]` â†’ att_linearâ†’ `[B, 66, L, 1]` â†’ squeezeâ†’ `[B, 66, L]`
- `att`: `[B, 66, L]` = softmaxåçš„æ³¨æ„åŠ›æƒé‡

---

### 5. æ–¹é¢ç›¸å…³ç‰¹å¾ h_t^A è®¡ç®— (å…¬å¼3)

#### è®ºæ–‡å…¬å¼
```
h_t^A = Î£(Î±_t,i Ã— h_i^CA)
ç»´åº¦: â„^d
```

#### ä»£ç å®ç°
```python
# MAESC_model.py:215
att_features = torch.matmul(att, noun_embed)
# ç»´åº¦: torch.matmul([B,66,L], [B,L,768]) â†’ [B,66,768]
```

**ç»´åº¦å˜åŒ–**:
- `att`: `[B, 66, L]`
- `noun_embed`: `[B, L, 768]`
- `att_features`: `[B, 66, 768]` = h_t^A

**æ•°å­¦è®¡ç®—**:
```
å¯¹äºç¬¬tä¸ªtoken (å›ºå®št):
h_t^A = Î±_t,1 Ã— h_1^CA + Î±_t,2 Ã— h_2^CA + ... + Î±_t,L Ã— h_L^CA
     = Î£(i=1 to L) Î±_t,i Ã— h_i^CA
```

---

### 6. èåˆç³»æ•° Î²_t è®¡ç®— (å…¬å¼4)

#### è®ºæ–‡å…¬å¼
```
Î²_t = sigmoid(W_Î² [W_1 h_t; W_2 h_t^A] + b_Î²)
ç»´åº¦: sigmoid(scalar) âˆˆ [0,1]
```

#### ä»£ç å®ç°
```python
# MAESC_model.py:217
alpha = torch.sigmoid(self.linear(
    torch.cat([self.alpha_linear1(encoder_outputs),
               self.alpha_linear2(att_features)], dim=-1)))
# ç»´åº¦:
# encoder_outputs: [B, 66, 768]
# alpha_linear1(encoder_outputs): [B, 66, 768]
# att_features: [B, 66, 768]
# alpha_linear2(att_features): [B, 66, 768]
# torch.cat([...], dim=-1): [B, 66, 1536]
# self.linear(...): [B, 66, 1]
# sigmoid: [B, 66, 1]
```

**ç»´åº¦å˜åŒ–**:
- `encoder_outputs`: `[B, 66, 768]` â†’ alpha_linear1â†’ `[B, 66, 768]`
- `att_features`: `[B, 66, 768]` â†’ alpha_linear2â†’ `[B, 66, 768]`
- æ‹¼æ¥: `[B, 66, 1536]`
- `alpha`: `[B, 66, 1]` = Î²_t

---

### 7. æœ€ç»ˆå¯¹é½ç‰¹å¾ Ä¥_t è®¡ç®— (å…¬å¼5)

#### è®ºæ–‡å…¬å¼
```
Ä¥_t = Î²_t Ã— h_t + (1-Î²_t) Ã— h_t^A
ç»´åº¦: â„^d
```

#### ä»£ç å®ç°
```python
# MAESC_model.py:217-220
alpha = alpha.repeat(1, 1, 768)  # å¹¿æ’­åˆ°768ç»´
# alpha: [B, 66, 1] â†’ [B, 66, 768]

encoder_outputs = torch.mul(1-alpha, encoder_outputs) + torch.mul(alpha, att_features)
# ç»´åº¦:
# (1-alpha): [B, 66, 768]
# encoder_outputs: [B, 66, 768]
# torch.mul(1-alpha, encoder_outputs): [B, 66, 768]
# alpha: [B, 66, 768]
# att_features: [B, 66, 768]
# torch.mul(alpha, att_features): [B, 66, 768]
# åŠ æ³•: [B, 66, 768] = Ä¥_t
```

**ç»´åº¦å˜åŒ–**:
- `alpha.repeat(1, 1, 768)`: `[B, 66, 1]` â†’ `[B, 66, 768]`
- `encoder_outputs`: `[B, 66, 768]` â†’ çº¿æ€§ç»„åˆâ†’ `[B, 66, 768]`

**æ•°å­¦è®¡ç®—**:
```
Ä¥_t = (1-Î²_t) Ã— h_t + Î²_t Ã— h_t^A
```

---

## äºŒã€AG-GCNæ¨¡å— - æ•°æ®ç»´åº¦å˜æ¢è¯¦è§£

### 1. å¤šæ¨¡æ€ç‰¹å¾åˆ†ç¦»

#### ä»£ç å®ç°
```python
# MAESC_model.py:249-250
img_feature = encoder_outputs[:, :51, :]  # å›¾åƒç‰¹å¾
text_feature = encoder_outputs[:, 51:, :]  # æ–‡æœ¬ç‰¹å¾
# ç»´åº¦:
# encoder_outputs: [B, 66, 768]
# img_feature: [B, 51, 768]  # 51ä¸ªå›¾åƒpatch
# text_feature: [B, 15, 768]  # 15ä¸ªæ–‡æœ¬token
```

---

### 2. å¸ƒå°”ä¾èµ–çŸ©é˜µ D æ„å»º (å…¬å¼9)

#### ä»£ç å®ç°
```python
# MAESC_model.py:248
new_dependency_matrix = torch.zeros([B, 66, 66], dtype=torch.float).to(encoder_outputs.device)
# ç»´åº¦: [B, 66, 66] = åˆ†å—çŸ©é˜µD

# è®¾ç½®å¯¹è§’çº¿ä¸º1 (è‡ªç¯)
for i in range(new_dependency_matrix.shape[1]):
    new_dependency_matrix[:, i, i] = 1
# D[i,i] = 1
```

---

### 3. æ–‡æœ¬-æ–‡æœ¬ä¾èµ– (D_TT å­çŸ©é˜µ)

#### è®ºæ–‡ä¾èµ–çŸ©é˜µ (å¥æ³•ä¾èµ–)
```python
# dependency_matrixæ¥è‡ªspaCyçš„å¥æ³•åˆ†æ
# ç»´åº¦: [B, 15, 15] (ä»…æ–‡æœ¬éƒ¨åˆ†)
```

#### ä»£ç åº”ç”¨ (å…¬å¼10éƒ¨åˆ†å®ç°)
```python
# MAESC_model.py:281-284
text_feature_extend1 = text_feature.unsqueeze(1).repeat(1, 15, 1, 1)
text_feature_extend2 = text_feature.unsqueeze(2).repeat(1, 1, 15, 1)
text_sim = torch.cosine_similarity(text_feature_extend1, text_feature_extend2, dim=-1)
# ç»´åº¦:
# text_feature: [B, 15, 768]
# text_feature_extend1: [B, 15, 15, 768]
# text_feature_extend2: [B, 15, 15, 768]
# text_sim: [B, 15, 15] = ä½™å¼¦ç›¸ä¼¼åº¦

new_dependency_matrix[:, 51:, 51:] = dependency_matrix * text_sim
# ç»´åº¦: [B, 15, 15] (å¡«å…¥D_TTå­çŸ©é˜µ)
```

**ç»´åº¦å˜åŒ–**:
- `text_feature`: `[B, 15, 768]`
- `text_sim`: `[B, 15, 15]` = F_cosine_similarity(...)
- `new_dependency_matrix[:, 51:, 51:]`: `[B, 15, 15]`

---

### 4. å›¾åƒ-æ–‡æœ¬ä¾èµ– (D_VT/D_TV å­çŸ©é˜µ)

#### ä»£ç å®ç° (å…¬å¼10éƒ¨åˆ†å®ç°)
```python
# MAESC_model.py:270-278
img_feature_extend = img_feature.unsqueeze(2).repeat(1, 1, text_feature.shape[1], 1)
text_feature_extend = text_feature.unsqueeze(1).repeat(1, img_feature.shape[1], 1, 1)
sim = torch.cosine_similarity(img_feature_extend, text_feature_extend, dim=-1)
# ç»´åº¦:
# img_feature: [B, 51, 768]
# text_feature: [B, 15, 768]
# img_feature_extend: [B, 51, 15, 768]
# text_feature_extend: [B, 51, 15, 768]
# sim: [B, 51, 15] = å›¾åƒ-æ–‡æœ¬ç›¸ä¼¼åº¦çŸ©é˜µ

# å›¾åƒåªä¸åè¯æŒ‚é’©
noun_mask = noun_mask[:, 51:].unsqueeze(1).repeat(1, sim.shape[1], 1)
sim = sim * noun_mask  # è¿‡æ»¤éåè¯ä¾èµ–

new_dependency_matrix[:, :51, 51:] = sim  # D_VT
new_dependency_matrix[:, 51:, :51] = torch.transpose(sim, 1, 2)  # D_TV
# ç»´åº¦: [B, 51, 15]
```

**ç»´åº¦å˜åŒ–**:
- `img_feature_extend`: `[B, 51, 15, 768]`
- `text_feature_extend`: `[B, 51, 15, 768]`
- `sim`: `[B, 51, 15]`
- `new_dependency_matrix[:, :51, 51:]`: `[B, 51, 15]`
- `new_dependency_matrix[:, 51:, :51]`: `[B, 15, 51]`

---

### 5. æƒ…æ„Ÿåˆ†æ•°è·å– (å…¬å¼6-7)

#### è®ºæ–‡å…¬å¼
```
w_i^S = SenticNet(w_i)
s_i = W_S w_i^S + b_S
ç»´åº¦: â„^d
```

#### ä»£ç å®ç°
```python
# MAESC_model.py:294-298
# å¡«å……å›¾åƒåŒºåŸŸæƒ…æ„Ÿå€¼ (é›¶å¡«å……)
sentiment_value = nn.ZeroPad2d(padding=(51, 0, 0, 0))(sentiment_value)
# sentiment_value: [B, 15] â†’ [B, 51+15] = [B, 66]
sentiment_value = sentiment_value.unsqueeze(-1)
# ç»´åº¦: [B, 66] â†’ [B, 66, 1]

sentiment_feature = self.senti_value_linear(sentiment_value)
# ç»´åº¦: [B, 66, 1] â†’ senti_value_linearâ†’ [B, 66, 768]
```

**ç»´åº¦å˜åŒ–**:
- `sentiment_value`: `[B, 15]` â†’ ZeroPad2dâ†’ `[B, 66]`
- `sentiment_feature`: `[B, 66, 1]` â†’ Linearâ†’ `[B, 66, 768]` = s_i

---

### 6. æƒ…æ„Ÿ-è¯­ä¹‰èåˆ (å…¬å¼8)

#### è®ºæ–‡å…¬å¼
```
h_i^S = Ä¥_i + s_i
ç»´åº¦: â„^d
```

#### ä»£ç å®ç°
```python
# MAESC_model.py:298
context_feature = self.context_linear(encoder_outputs + sentiment_feature)
# ç»´åº¦:
# encoder_outputs: [B, 66, 768] = Ä¥_i
# sentiment_feature: [B, 66, 768] = s_i
# åŠ æ³•: [B, 66, 768]
# context_linear: [B, 66, 768] â†’ [B, 66, 768] (çº¿æ€§å˜æ¢)
```

**ç»´åº¦å˜åŒ–**:
- `encoder_outputs`: `[B, 66, 768]`
- `sentiment_feature`: `[B, 66, 768]`
- `encoder_outputs + sentiment_feature`: `[B, 66, 768]`
- `context_feature`: `[B, 66, 768]` = h_i^S

---

### 7. å›¾å·ç§¯ç‰¹å¾æ›´æ–° (å…¬å¼11)

#### ä»£ç å®ç°
```python
# MAESC_model.py:299
context_feature = self.context_gcn(context_feature, context_dependency_matrix, attention_mask)
```

**GCNå†…éƒ¨å®ç°** (æ¥è‡ª `GCN` ç±»):
```python
# GCN.py ä¸­çš„å›¾å·ç§¯è®¡ç®—
def forward(self, inputs, adj, mask=None):
    # inputs: [B, 66, 768]
    # adj: [B, 66, 66]

    support = torch.matmul(inputs, self.weight)
    # support: [B, 66, 768] @ [768, 768] â†’ [B, 66, 768]

    output = torch.matmul(adj, support)
    # output: [B, 66, 66] @ [B, 66, 768] â†’ [B, 66, 768]

    output = self.act(output)  # ReLU
    return output
```

**ç»´åº¦å˜åŒ–**:
- `inputs`: `[B, 66, 768]`
- `adj`: `[B, 66, 66]` = åŠ æƒå…³è”çŸ©é˜µA
- `support`: `[B, 66, 768]`
- `output`: `[B, 66, 768]` = h_i,l^S (ç¬¬lå±‚GCNè¾“å‡º)

---

### 8. æœ€ç»ˆèåˆç‰¹å¾ (å…¬å¼12)

#### è®ºæ–‡å…¬å¼
```
HÌƒ = Î»_1 Ã— Ä¤ + Î»_2 Ã— Ä¤^S
```

#### ä»£ç å®ç°
```python
# MAESC_model.py:302
mix_feature = self.gcn_proportion * context_feature + encoder_outputs
# ç»´åº¦:
# context_feature: [B, 66, 768] = Ä¤^S
# encoder_outputs: [B, 66, 768] = Ä¤
# gcn_proportion: scalar = 0.5
# mix_feature: [B, 66, 768] = HÌƒ
```

**ç»´åº¦å˜åŒ–**:
- `context_feature`: `[B, 66, 768]`
- `encoder_outputs`: `[B, 66, 768]`
- `mix_feature`: `[B, 66, 768]`

---

## ä¸‰ã€é¢„æµ‹æ¨¡å— - æ•°æ®ç»´åº¦å˜æ¢

### 1. è§£ç å™¨è¾“å…¥

#### ä»£ç å®ç°
```python
# MAESC_model.py:237-243
dict = self.decoder(input_ids=tokens,
                    encoder_hidden_states=mix_feature,
                    encoder_padding_mask=encoder_pad_mask,
                    decoder_padding_mask=decoder_pad_mask,
                    decoder_causal_mask=self.causal_masks[:tokens.size(1), :tokens.size(1)],
                    return_dict=True)

hidden_state = dict.last_hidden_state
# ç»´åº¦: [B, max_len, 768] = h_t^d (å…¬å¼13)
```

**ç»´åº¦å˜åŒ–**:
- `mix_feature`: `[B, 66, 768]` â†’ BART Decoderâ†’ `hidden_state`: `[B, max_len, 768]`

---

### 2. é¢„æµ‹æ¦‚ç‡è®¡ç®— (å…¬å¼15)

#### ä»£ç å®ç°
```python
# MAESC_model.py:273-278
tag_scores = F.linear(
    hidden_state,
    self.dropout_layer(
        self.decoder.embed_tokens.weight[self.label_start_id:self.label_start_id + 3]))
# ç»´åº¦:
# hidden_state: [B, max_len, 768]
# embed_tokens.weight: [vocab_size, 768]
# labeléƒ¨åˆ†æƒé‡: [3, 768] (POS, NEU, NEG)
# F.linear: [B, max_len, 768] @ [3, 768]^T â†’ [B, max_len, 3]

logits[:, :, 3:self.src_start_index] = tag_scores
# ç»´åº¦: [B, max_len, num_classes]
```

**ç»´åº¦å˜åŒ–**:
- `hidden_state`: `[B, max_len, 768]`
- `tag_scores`: `[B, max_len, 3]`
- `logits`: `[B, max_len, num_classes]` = P(y_t)

---

## å››ã€ç»´åº¦å˜åŒ–æµç¨‹å›¾

```
è¾“å…¥: [B, 66, 768]
  â†“
åè¯æå– â†’ noun_embed: [B, L, 768]
  â†“
AÂ³Mæ¨¡å—:
  multi_features_rep: [B, 66, L, 768]
  noun_features_rep: [B, 66, L, 768]
  â†“
concat_features: [B, 66, L, 1536]
  â†“
att: [B, 66, L] (softmax)
  â†“
att_features: [B, 66, 768] = h_t^A
  â†“
alpha: [B, 66, 1] â†’ [B, 66, 768] (Î²_t)
  â†“
encoder_outputs: [B, 66, 768] = Ä¥_t
  â†“
AG-GCNæ¨¡å—:
  img_feature: [B, 51, 768]
  text_feature: [B, 15, 768]
  â†“
dependency_matrix: [B, 66, 66] (D)
  â†“
text_sim: [B, 15, 15]
  â†“
sim (img-text): [B, 51, 15]
  â†“
new_dependency_matrix: [B, 66, 66] (A)
  â†“
sentiment_feature: [B, 66, 768] (s_i)
  â†“
context_feature: [B, 66, 768] = h_i^S
  â†“
GCN(context_feature): [B, 66, 768] = Ä¤^S
  â†“
mix_feature: [B, 66, 768] = HÌƒ
  â†“
Decoder â†’ hidden_state: [B, max_len, 768] = h_t^d
  â†“
tag_scores: [B, max_len, 3]
  â†“
logits: [B, max_len, num_classes] = P(y_t)
```

---

## äº”ã€å…³é”®å¼ é‡ç»´åº¦æ€»ç»“

| å¼ é‡åç§° | ç»´åº¦ | å«ä¹‰ |
|----------|------|------|
| `encoder_outputs` | `[B, 66, 768]` | BARTç¼–ç è¾“å‡º |
| `noun_embed` | `[B, L, 768]` | å€™é€‰æ–¹é¢ç‰¹å¾ H^CA |
| `att` | `[B, 66, L]` | æ³¨æ„åŠ›æƒé‡ Î±_t |
| `att_features` | `[B, 66, 768]` | æ–¹é¢ç›¸å…³ç‰¹å¾ h_t^A |
| `alpha` | `[B, 66, 768]` | èåˆç³»æ•° Î²_t |
| `img_feature` | `[B, 51, 768]` | å›¾åƒç‰¹å¾ |
| `text_feature` | `[B, 15, 768]` | æ–‡æœ¬ç‰¹å¾ |
| `dependency_matrix` | `[B, 66, 66]` | å¥æ³•ä¾èµ–çŸ©é˜µ D |
| `sim` | `[B, 51, 15]` | å›¾åƒ-æ–‡æœ¬ç›¸ä¼¼åº¦ |
| `sentiment_feature` | `[B, 66, 768]` | æƒ…æ„Ÿç‰¹å¾ s_i |
| `context_feature` | `[B, 66, 768]` | èåˆåç‰¹å¾ h_i^S |
| `mix_feature` | `[B, 66, 768]` | æœ€ç»ˆèåˆç‰¹å¾ HÌƒ |
| `hidden_state` | `[B, max_len, 768]` | è§£ç å™¨è¾“å‡º |
| `logits` | `[B, max_len, num_classes]` | é¢„æµ‹æ¦‚ç‡ |

---

## å…­ã€ä»£ç å…³é”®ä½ç½®

1. **åè¯æå–**: `MAESC_model.py:141-159` (`get_noun_embed`)
2. **AÂ³Mæ³¨æ„åŠ›**: `MAESC_model.py:207-244` (`noun_attention`)
3. **å¤šæ¨¡æ€GCN**: `MAESC_model.py:246-304` (`multimodal_GCN`)
4. **GCNå®ç°**: `src/model/GCN.py` (å›¾å·ç§¯å±‚)
5. **æƒ…æ„Ÿç‰¹å¾**: `MAESC_model.py:209` (`senti_value_linear`)

---

**åˆ†æå®Œæˆæ—¶é—´**: 2025-11-13
**è®ºæ–‡å…¬å¼ä¸ä»£ç ç»´åº¦å®Œå…¨å¯¹åº”**: âœ…