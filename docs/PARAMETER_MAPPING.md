# AoMè®ºæ–‡å…¬å¼å‚æ•°ä¸ä»£ç å®ç°æ˜ å°„

> æœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜AoMè®ºæ–‡ä¸­æ¯ä¸ªå…¬å¼çš„å‚æ•°åœ¨ä»£ç ä¸­çš„å…·ä½“å®ç°ã€‚

---

## ğŸ“Š å‚æ•°æ˜ å°„æ€»è§ˆ

| è®ºæ–‡å‚æ•° | ä»£ç å®ç° | ç»´åº¦ | ä½ç½® |
|----------|----------|------|------|
| W_CA, b_CA | `self.noun_linear` | 768â†’768 | MAESC_model.py:115 |
| W_H, b_H | `self.multi_linear` | 768â†’768 | MAESC_model.py:115 |
| W_Î±, b_Î± | `self.att_linear` | 1536â†’1 | MAESC_model.py:116 |
| W_1, W_2 | `self.alpha_linear1`, `self.alpha_linear2` | 768â†’768 | MAESC_model.py:121-122 |
| W_Î², b_Î² | `self.linear` | 1536â†’1 | MAESC_model.py:118 |
| W_S, b_S | `self.senti_value_linear` | 1â†’768 | MAESC_model.py:135 |
| W_context | `self.context_linear` | 768â†’768 | MAESC_model.py:125 |
| W_l | GCNå†…éƒ¨æƒé‡ | 768â†’768 | GCN.py |
| Î»_1, Î»_2 | `self.gcn_proportion` | scalar | MAESC_model.py:100 |

---

## ä¸€ã€AÂ³Mæ¨¡å—å‚æ•°è¯¦è§£

### 1. ç»¼åˆç‰¹å¾ Z_t è®¡ç®— (å…¬å¼1)

#### è®ºæ–‡å‚æ•°
```
Z_t = tanh((W_CA H^CA + b_CA) âŠ• (W_H h_t + b_H))
```

#### ä»£ç å‚æ•°
```python
# MAESC_model.py:115
self.noun_linear = nn.Linear(768, 768)  # W_CA, b_CA (PyTorchè‡ªåŠ¨æ·»åŠ bias)
self.multi_linear = nn.Linear(768, 768)  # W_H, b_H (PyTorchè‡ªåŠ¨æ·»åŠ bias)
```

#### å‚æ•°ç»†èŠ‚
```python
# noun_linearæƒé‡
noun_linear.weight: [768, 768]  # W_CA
noun_linear.bias: [768]          # b_CA

# multi_linearæƒé‡
multi_linear.weight: [768, 768]  # W_H
multi_linear.bias: [768]          # b_H
```

#### æ¿€æ´»å‡½æ•°
```python
# ä»£ç å®ç° (MAESC_model.py:213)
concat_features = torch.tanh(torch.cat([noun_features_rep, multi_features_rep], dim=-1))
# å¯¹åº”è®ºæ–‡çš„ tanh(...) éƒ¨åˆ†
```

---

### 2. æ³¨æ„åŠ›åˆ†å¸ƒ Î±_t è®¡ç®— (å…¬å¼2)

#### è®ºæ–‡å‚æ•°
```
Î±_t = softmax(W_Î± Z_t + b_Î±)
```

#### ä»£ç å‚æ•°
```python
# MAESC_model.py:116
self.att_linear = nn.Linear(768*2, 1)  # W_Î±, b_Î±
```

#### å‚æ•°ç»†èŠ‚
```python
# att_linearæƒé‡
att_linear.weight: [1, 1536]  # W_Î± (1è¡Œ1536åˆ—)
att_linear.bias: [1]           # b_Î±
```

#### ä»£ç å®ç°
```python
# MAESC_model.py:214
att = torch.softmax(self.att_linear(concat_features).squeeze(-1), dim=-1)
```

**ç»´åº¦å˜åŒ–**:
```
concat_features: [B, 66, L, 1536]
att_linear: [1, 1536] â†’ [B, 66, L, 1]
.squeeze(-1): [B, 66, L]
softmax: [B, 66, L] = Î±_t
```

---

### 3. æ–¹é¢ç›¸å…³ç‰¹å¾ h_t^A è®¡ç®— (å…¬å¼3)

#### è®ºæ–‡å‚æ•°
```
h_t^A = Î£(Î±_t,i Ã— h_i^CA)
```

#### ä»£ç å®ç°
```python
# MAESC_model.py:215
att_features = torch.matmul(att, noun_embed)
```

**çŸ©é˜µä¹˜æ³•è¯¦è§£**:
```
att: [B, 66, L] = [[Î±_1, Î±_2, ..., Î±_L] for each t]
noun_embed: [B, L, 768] = [[h_1^CA, h_2^CA, ..., h_L^CA]]

torch.matmul([B,66,L], [B,L,768]) = [B,66,768]

æ•°å­¦è®¡ç®—:
h_t^A = Î±_t,1 Ã— h_1^CA + Î±_t,2 Ã— h_2^CA + ... + Î±_t,L Ã— h_L^CA
```

---

### 4. èåˆç³»æ•° Î²_t è®¡ç®— (å…¬å¼4)

#### è®ºæ–‡å‚æ•°
```
Î²_t = sigmoid(W_Î² [W_1 h_t; W_2 h_t^A] + b_Î²)
```

#### ä»£ç å‚æ•°
```python
# MAESC_model.py:121-122
self.alpha_linear1 = nn.Linear(768, 768)  # W_1
self.alpha_linear2 = nn.Linear(768, 768)  # W_2

# MAESC_model.py:118
self.linear = nn.Linear(768*2, 1)  # W_Î², b_Î²
```

#### å‚æ•°ç»†èŠ‚
```python
# W_1, W_2
alpha_linear1.weight: [768, 768]
alpha_linear1.bias: [768]

alpha_linear2.weight: [768, 768]
alpha_linear2.bias: [768]

# W_Î², b_Î²
linear.weight: [1, 1536]
linear.bias: [1]
```

#### ä»£ç å®ç°
```python
# MAESC_model.py:217
alpha = torch.sigmoid(self.linear(
    torch.cat([self.alpha_linear1(encoder_outputs),
               self.alpha_linear2(att_features)], dim=-1)))
```

**è®¡ç®—è¿‡ç¨‹**:
```
æ­¥éª¤1: W_1 h_t
encoder_outputs: [B, 66, 768]
alpha_linear1: [768, 768] â†’ [B, 66, 768]

æ­¥éª¤2: W_2 h_t^A
att_features: [B, 66, 768]
alpha_linear2: [768, 768] â†’ [B, 66, 768]

æ­¥éª¤3: [W_1 h_t; W_2 h_t^A]
torch.cat([...], dim=-1): [B, 66, 1536]

æ­¥éª¤4: W_Î² [W_1 h_t; W_2 h_t^A] + b_Î²
linear: [1, 1536] â†’ [B, 66, 1]
sigmoid: [B, 66, 1] = Î²_t
```

---

### 5. æœ€ç»ˆå¯¹é½ç‰¹å¾ Ä¥_t è®¡ç®— (å…¬å¼5)

#### è®ºæ–‡å‚æ•°
```
Ä¥_t = Î²_t Ã— h_t + (1-Î²_t) Ã— h_t^A
```

#### ä»£ç å®ç°
```python
# MAESC_model.py:217-220
alpha = alpha.repeat(1, 1, 768)  # å¹¿æ’­Î²_tåˆ°768ç»´
encoder_outputs = torch.mul(1-alpha, encoder_outputs) + torch.mul(alpha, att_features)
```

**è®¡ç®—ç»†èŠ‚**:
```python
# alpha (Î²_t): [B, 66, 1] â†’ [B, 66, 768] (å¹¿æ’­)
# encoder_outputs (h_t): [B, 66, 768]
# att_features (h_t^A): [B, 66, 768]

# (1-Î²_t) Ã— h_t
torch.mul(1-alpha, encoder_outputs): [B, 66, 768]

# Î²_t Ã— h_t^A
torch.mul(alpha, att_features): [B, 66, 768]

# ç›¸åŠ 
result: [B, 66, 768] = Ä¥_t
```

---

## äºŒã€AG-GCNæ¨¡å—å‚æ•°è¯¦è§£

### 6. æƒ…æ„Ÿåˆ†æ•°è·å– (å…¬å¼6-7)

#### è®ºæ–‡å‚æ•°
```
w_i^S = SenticNet(w_i)
s_i = W_S w_i^S + b_S
```

#### ä»£ç å‚æ•°
```python
# MAESC_model.py:135
self.senti_value_linear = nn.Linear(1, 768)  # W_S, b_S
```

#### å‚æ•°ç»†èŠ‚
```python
senti_value_linear.weight: [768, 1]  # W_S
senti_value_linear.bias: [768]        # b_S
```

#### ä»£ç å®ç°
```python
# MAESC_model.py:294-297
sentiment_value = nn.ZeroPad2d(padding=(51, 0, 0, 0))(sentiment_value)
# [B, 15] â†’ [B, 66]

sentiment_value = sentiment_value.unsqueeze(-1)
# [B, 66] â†’ [B, 66, 1]

sentiment_feature = self.senti_value_linear(sentiment_value)
# [B, 66, 1] â†’ senti_value_linear â†’ [B, 66, 768] = s_i
```

**è®¡ç®—è¿‡ç¨‹**:
```
æ­¥éª¤1: è·å–SenticNetåˆ†æ•°
w_i^S: [B, 15] (æ¥è‡ªsenticNetè¯å…¸)

æ­¥éª¤2: å¡«å……å›¾åƒåŒºåŸŸ
w_i^S: [B, 15] â†’ ZeroPad2d â†’ [B, 66]

æ­¥éª¤3: W_S w_i^S + b_S
unsqueeze: [B, 66] â†’ [B, 66, 1]
Linear: [B, 66, 1] @ [768, 1]^T + [768] â†’ [B, 66, 768]
```

---

### 7. æƒ…æ„Ÿ-è¯­ä¹‰èåˆ (å…¬å¼8)

#### è®ºæ–‡å‚æ•°
```
h_i^S = Ä¥_i + s_i
```

#### ä»£ç å®ç°
```python
# MAESC_model.py:298
context_feature = self.context_linear(encoder_outputs + sentiment_feature)
```

#### ä»£ç å‚æ•°
```python
# MAESC_model.py:125
self.context_linear = nn.Linear(768, 768)  # W_context (çº¿æ€§å˜æ¢å±‚)
```

**è®¡ç®—è¿‡ç¨‹**:
```
encoder_outputs (Ä¥_i): [B, 66, 768]
sentiment_feature (s_i): [B, 66, 768]

åŠ æ³•: [B, 66, 768] + [B, 66, 768] = [B, 66, 768]

Linearå˜æ¢:
input: [B, 66, 768]
context_linear.weight: [768, 768]
context_linear.bias: [768]
output: [B, 66, 768] = h_i^S
```

---

### 8. å›¾å·ç§¯æƒé‡ (å…¬å¼11)

#### è®ºæ–‡å‚æ•°
```
h_{i,l}^S = ReLU(Î£ A_{ij} W_l h_{i,l-1}^S + b_l)
```

#### ä»£ç å‚æ•° (GCNç±»)
```python
# GCN.py
self.weight = nn.Parameter(torch.FloatTensor(in_dim, out_dim))
# é»˜è®¤: in_dim=768, out_dim=768
```

#### GCNå®ç°ç»†èŠ‚
```python
# GCN.py å›¾å·ç§¯è®¡ç®—
def forward(self, inputs, adj, mask=None):
    # inputs: [B, 66, 768] = h_{i,l-1}^S
    # adj: [B, 66, 66] = A (å…³è”çŸ©é˜µ)

    # æ­¥éª¤1: W_l h_{i,l-1}^S
    support = torch.matmul(inputs, self.weight)
    # [B, 66, 768] @ [768, 768] â†’ [B, 66, 768]

    # æ­¥éª¤2: Î£ A_{ij} W_l h_{i,l-1}^S
    output = torch.matmul(adj, support)
    # [B, 66, 66] @ [B, 66, 768] â†’ [B, 66, 768]

    # æ­¥éª¤3: ReLU(W_l h + b_l)
    output = self.act(output + self.bias)  # ReLU
    return output
```

#### å‚æ•°ç»†èŠ‚
```python
# GCNæƒé‡ (MAESC_model.py:127-128)
self.context_gcn = GCN(768, 768, 768, dropout=self.gcn_dropout)
# è¾“å…¥ç»´åº¦: 768, è¾“å‡ºç»´åº¦: 768, éšè—ç»´åº¦: 768
```

---

### 9. æœ€ç»ˆèåˆæƒé‡ (å…¬å¼12)

#### è®ºæ–‡å‚æ•°
```
HÌƒ = Î»_1 Ã— Ä¤ + Î»_2 Ã— Ä¤^S
```

#### ä»£ç å‚æ•°
```python
# MAESC_model.py:100
self.gcn_proportion = args.gcn_proportion  # é»˜è®¤ 0.5
```

#### ä»£ç å®ç°
```python
# MAESC_model.py:302
mix_feature = self.gcn_proportion * context_feature + encoder_outputs
```

**è®¡ç®—è¿‡ç¨‹**:
```python
context_feature (Ä¤^S): [B, 66, 768]
encoder_outputs (Ä¤): [B, 66, 768]
gcn_proportion (Î»_2): 0.5 (é»˜è®¤)

# Î»_1 = 1 - Î»_2 = 0.5
mix_feature = 0.5 Ã— context_feature + 1.0 Ã— encoder_outputs
# [B, 66, 768] = HÌƒ
```

---

## ä¸‰ã€å¤šæ¨¡æ€ä¾èµ–çŸ©é˜µå‚æ•°

### 10. æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®— (dep_mode='text_cosine')

#### è®ºæ–‡å‚æ•° (å…¬å¼10éƒ¨åˆ†)
```
A_{ij} = D_{ij} Ã— F_cosine_similarity(Ä¥_i, Ä¥_j)
```

#### ä»£ç å‚æ•°
```python
# æ— æ˜¾å¼å‚æ•° (ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦å‡½æ•°)
torch.cosine_similarity(input1, input2, dim=-1)
```

#### ä»£ç å®ç°
```python
# MAESC_model.py:254-258
text_feature_extend1 = text_feature.unsqueeze(1).repeat(1, 15, 1, 1)
text_feature_extend2 = text_feature.unsqueeze(2).repeat(1, 1, 15, 1)
text_sim = torch.cosine_similarity(text_feature_extend1, text_feature_extend2, dim=-1)
new_dependency_matrix[:, 51:, 51:] = dependency_matrix * text_sim
```

**è®¡ç®—ç»†èŠ‚**:
```python
text_feature: [B, 15, 768]
text_feature_extend1: [B, 15, 15, 768]
text_feature_extend2: [B, 15, 15, 768]

ä½™å¼¦ç›¸ä¼¼åº¦:
cos(Î¸) = (Ä¥_i Â· Ä¥_j) / (|Ä¥_i| Ã— |Ä¥_j|)
text_sim: [B, 15, 15]

æœ€ç»ˆä¾èµ–çŸ©é˜µ:
A_TT = D_TT Ã— text_sim
```

---

### 11. å›¾åƒ-æ–‡æœ¬å…³è” (dep_mode='text_cos_img_noun_sim')

#### è®ºæ–‡å‚æ•° (å…¬å¼10éƒ¨åˆ†)
```
A_{ij} = D_{ij} Ã— F_cosine_similarity(Ä¥_img_i, Ä¥_text_j) Ã— mask_noun_j
```

#### ä»£ç å‚æ•°
```python
# æ— æ˜¾å¼å‚æ•° (ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦)
torch.cosine_similarity(...)
```

#### ä»£ç å®ç°
```python
# MAESC_model.py:270-278
img_feature_extend = img_feature.unsqueeze(2).repeat(1, 1, text_feature.shape[1], 1)
text_feature_extend = text_feature.unsqueeze(1).repeat(1, img_feature.shape[1], 1, 1)
sim = torch.cosine_similarity(img_feature_extend, text_feature_extend, dim=-1)

noun_mask = noun_mask[:, 51:].unsqueeze(1).repeat(1, sim.shape[1], 1)
sim = sim * noun_mask

new_dependency_matrix[:, :51, 51:] = sim
new_dependency_matrix[:, 51:, :51] = torch.transpose(sim, 1, 2)
```

**è®¡ç®—ç»†èŠ‚**:
```python
img_feature: [B, 51, 768]
text_feature: [B, 15, 768]

å›¾åƒ-æ–‡æœ¬ç›¸ä¼¼åº¦:
img_feature_extend: [B, 51, 15, 768]
text_feature_extend: [B, 51, 15, 768]
sim = cos(Ä¥_img, Ä¥_text): [B, 51, 15]

åè¯è¿‡æ»¤:
noun_mask: [B, 15] (0æˆ–1)
è¿‡æ»¤å: [B, 51, 15]

æœ€ç»ˆä¾èµ–çŸ©é˜µ:
A_VT = sim: [B, 51, 15]
A_TV = sim^T: [B, 15, 51]
```

---

## å››ã€å‚æ•°åˆå§‹åŒ–æ–¹å¼

### PyTorché»˜è®¤åˆå§‹åŒ–

æ‰€æœ‰çº¿æ€§å±‚ä½¿ç”¨PyTorchçš„é»˜è®¤åˆå§‹åŒ–ï¼š

```python
# nn.Linearçš„é»˜è®¤åˆå§‹åŒ–
nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))
fan_in = self.in_features
std = 1.0 / math.sqrt(fan_in)
bound = math.sqrt(3.0) * std
nn.init.uniform_(self.bias, -bound, bound)
```

### å…³é”®å‚æ•°å€¼

| å‚æ•°å | åˆå§‹åŒ–å€¼ | è¯´æ˜ |
|--------|----------|------|
| æ‰€æœ‰Linearæƒé‡ | Kaimingåˆå§‹åŒ– | PyTorché»˜è®¤ |
| æ‰€æœ‰Linearåç½® | å‡åŒ€åˆ†å¸ƒ | PyTorché»˜è®¤ |
| GCNæƒé‡ | Xavieråˆå§‹åŒ– | GCN.pyä¸­å®šä¹‰ |
| gcn_proportion | 0.5 | è¶…å‚æ•°ï¼Œå¯è°ƒæ•´ |
| dropout | 0.0-0.3 | ä¸åŒå±‚ä½¿ç”¨ä¸åŒå€¼ |

---

## äº”ã€ä»£ç å‚æ•°ä½ç½®ç´¢å¼•

### MAESC_model.py å‚æ•°å®šä¹‰
```python
# ç¬¬115-138è¡Œ
self.noun_linear           # W_CA, b_CA
self.multi_linear          # W_H, b_H
self.att_linear            # W_Î±, b_Î±
self.alpha_linear1         # W_1
self.alpha_linear2         # W_2
self.linear                # W_Î², b_Î²
self.context_linear        # W_context
self.senti_value_linear    # W_S, b_S
self.dep_linear1           # ä¾èµ–è®¡ç®—
self.dep_linear2           # ä¾èµ–è®¡ç®—
self.dep_att_linear        # ä¾èµ–è®¡ç®—
```

### GCN.py å‚æ•°å®šä¹‰
```python
# å›¾å·ç§¯å±‚æƒé‡
self.weight = nn.Parameter(torch.FloatTensor(in_dim, out_dim))
self.bias = nn.Parameter(torch.FloatTensor(out_dim))
```

---

## å…­ã€è®­ç»ƒå‚æ•°æ€»ç»“

### 1. è¶…å‚æ•°
```python
args.gcn_proportion = 0.5      # Î»_2
args.gcn_dropout = 0.0         # GCN dropout
args.nn_attention_mode = 'cat' # AÂ³Mæ¨¡å¼é€‰æ‹©
```

### 2. å­¦ä¹ å‚æ•°
- æ‰€æœ‰çº¿æ€§å±‚æƒé‡é€šè¿‡æ¢¯åº¦ä¸‹é™ä¼˜åŒ–
- æ— é¢„è®­ç»ƒå‚æ•°åˆå§‹åŒ–
- ä¼˜åŒ–å™¨: AdamW (é»˜è®¤é…ç½®)

### 3. æ­£åˆ™åŒ–
```python
dropout: 0.1-0.3 (ä¸åŒå±‚)
grad_clip: 5.0
layer_drop: 0.0 (Transformerå±‚)
```

---

**åˆ†æå®Œæˆæ—¶é—´**: 2025-11-13
**å‚æ•°æ˜ å°„å®Œæ•´æ€§**: âœ…